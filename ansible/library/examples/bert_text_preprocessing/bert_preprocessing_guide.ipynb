{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tce3stUlHN0L",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Copyright 2021 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "tuOe1ymfHZPu"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFdPvlXBOdUN"
   },
   "source": [
    "# BERT Preprocessing with TF Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfBg1C5NB3X0"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/text/guide/bert_preprocessing_guide\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/text/blob/master/docs/guide/bert_preprocessing_guide.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/text/blob/master/docs/guide/bert_preprocessing_guide.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/text/docs/guide/bert_preprocessing_guide.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHxb-dlhMIzW"
   },
   "source": [
    "## Overview\n",
    "\n",
    "Text preprocessing is the end-to-end transformation of raw text into a modelâ€™s integer inputs. NLP models are often accompanied by several hundreds (if not thousands) of lines of Python code for preprocessing text. Text preprocessing is often a challenge for models because:\n",
    "\n",
    "* **Training-serving skew.** It becomes increasingly difficult to ensure that the preprocessing logic of the model's inputs are consistent at all stages of model development (e.g. pretraining, fine-tuning, evaluation, inference). \n",
    "Using different hyperparameters, tokenization, string preprocessing algorithms or simply packaging model inputs inconsistently at different stages could yield hard-to-debug and disastrous effects to the model. \n",
    "\n",
    "* **Efficiency and flexibility.** While preprocessing can be done offline (e.g. by writing out processed outputs to files on disk and then reconsuming said preprocessed data in the input pipeline), this method incurs an additional file read and write cost. Preprocessing offline is also inconvenient if there are preprocessing decisions that need to happen dynamically. Experimenting with a different option would require regenerating the dataset again.\n",
    "\n",
    "* **Complex model interface.** Text models are much more understandable when their inputs are pure text. It's hard to understand a model when its inputs require an extra, indirect encoding step. Reducing the preprocessing complexity is especially appreciated for model debugging, serving, and evaluation. \n",
    "\n",
    "Additionally, simpler model interfaces also make it more convenient to try the model (e.g. inference or training) on different, unexplored datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6DTHtXbxPgw"
   },
   "source": [
    "## Text preprocessing with TF.Text\n",
    "\n",
    "Using TF.Text's text preprocessing APIs, we can construct a preprocessing\n",
    "function that can transform a user's text dataset into the model's\n",
    "integer inputs. Users can package preprocessing directly as part of their model to alleviate the above mentioned problems.\n",
    "\n",
    "This tutorial will show how to use TF.Text preprocessing ops to transform text data into inputs for the BERT model and inputs for language masking pretraining task described in \"Masked LM and Masking Procedure\" of [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf). The process involves tokenizing text into subword units, combining sentences, trimming content to a fixed size and extracting labels for the masked language modeling task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUXex9ctTuDB"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmIjNKsfeTpm"
   },
   "source": [
    "Let's import the packages and libraries we need first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gTWQ5swI7FRJ"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U \"tensorflow-text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IqR2PQG4ZaZ0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "import functools\n",
    "print(\"TensorFlow version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-brDHSrRaMii"
   },
   "source": [
    "Our data contains two text features and we can create a example `tf.data.Dataset`. Our goal is to create a function that we can supply `Dataset.map()` with to be used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "DQyj7OQ9yk7K"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_a': <tf.Tensor: shape=(), dtype=string, numpy=b''>,\n",
       " 'text_b': <tf.Tensor: shape=(), dtype=string, numpy=b'\\n2023-10-03 03:23:59'>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = {\n",
    "'text_a':\n",
    "\"\"\"\n",
    "\"\"\".split(\",\")[:122],\n",
    "}\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(examples)\n",
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1laUIs3g5Qsz"
   },
   "source": [
    "### Tokenizing\n",
    "\n",
    "Our first step is to run any string preprocessing and tokenize our dataset. This can be done using the [`text.BertTokenizer`](https://tensorflow.org/text/api_docs/python/text/BertTokenizer), which is a [`text.Splitter`](https://tensorflow.org/text/api_docs/python/text/Splitter) that can tokenize sentences into subwords or wordpieces for the [BERT model](https://github.com/google-research/bert) given a vocabulary generated from the [Wordpiece algorithm](https://www.tensorflow.org/text/guide/subwords_tokenizer#optional_the_algorithm). You can learn more about other subword tokenizers available in TF.Text from [here](https://www.tensorflow.org/text/guide/subwords_tokenizer). \n",
    "\n",
    "\n",
    "The vocabulary can be from a previously generated BERT checkpoint, or you can generate one yourself on your own data. For the purposes of this example, let's create a toy vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ChpIFy515S1z"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "#os.system(\"/tmp/tok_vocab.txt\")\n",
    "pathlib.Path('/tmp/tok_vocab.txt').write_text(\n",
    "\"\"\"Jannah ## ##SER path 2023-10-03 03:24:14 575 p=2848 u=osmanjalloh n=ansible | <localhost> EXEC /bin/sh -c 'chmod u+x /Users/osmanjalloh/.ansible/tmp/ansible-tmp-1696328653.473581-2988-175545752765697/ /Users/osmanjalloh/.ansible/tmp/ansible-tmp-1696328653.473581-2988-175545752765697/AnsiballZ_command.py && sleep 0'\"\"\".replace(' ', '\\n'))\n",
    "\n",
    "lookup_table = '/tmp/tok_vocab.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7t2tgbSn6nvX"
   },
   "source": [
    "Let's construct a [`text.BertTokenizer`](https://tensorflow.org/text/api_docs/python/text/BertTokenizer) using the above vocabulary and tokenize the text inputs into a [`RaggedTensor`](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor).`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "564UPrFB5Zm6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-07 12:49:31.512239: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at lookup_table_init_op.cc:152 : FAILED_PRECONDITION: HashTable has different value for same key. Key 2023-10-03 has 6 and trying to add value 4\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "{{function_node __wrapped__InitializeTableFromTextFileV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} HashTable has different value for same key. Key 2023-10-03 has 6 and trying to add value 4 [Op:InitializeTableFromTextFileV2] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bert_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBertTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlookup_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_out_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m bert_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_a\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/IdeaProjects/debug/operator/jannah-python/lib/python3.11/site-packages/tensorflow_text/python/ops/bert_tokenizer.py:229\u001b[0m, in \u001b[0;36mBertTokenizer.__init__\u001b[0;34m(self, vocab_lookup_table, suffix_indicator, max_bytes_per_word, max_chars_per_token, token_out_type, unknown_token, split_unknown_characters, lower_case, keep_whitespace, normalization_form, preserve_unused_token, basic_tokenizer_class)\u001b[0m\n\u001b[1;32m    224\u001b[0m _tf_text_bert_tokenizer_op_create_counter\u001b[38;5;241m.\u001b[39mget_cell()\u001b[38;5;241m.\u001b[39mincrease_by(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_basic_tokenizer \u001b[38;5;241m=\u001b[39m basic_tokenizer_class(lower_case, keep_whitespace,\n\u001b[1;32m    227\u001b[0m                                               normalization_form,\n\u001b[1;32m    228\u001b[0m                                               preserve_unused_token)\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wordpiece_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mWordpieceTokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_lookup_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix_indicator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bytes_per_word\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_chars_per_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_out_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munknown_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_unknown_characters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/IdeaProjects/debug/operator/jannah-python/lib/python3.11/site-packages/tensorflow_text/python/ops/wordpiece_tokenizer.py:147\u001b[0m, in \u001b[0;36mWordpieceTokenizer.__init__\u001b[0;34m(self, vocab_lookup_table, suffix_indicator, max_bytes_per_word, max_chars_per_token, token_out_type, unknown_token, split_unknown_characters)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(vocab_lookup_table, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(vocab_lookup_table, tensor\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     vocab_lookup_table\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mstring):\n\u001b[1;32m    146\u001b[0m   init \u001b[38;5;241m=\u001b[39m lookup_ops\u001b[38;5;241m.\u001b[39mTextFileIdTableInitializer(vocab_lookup_table)\n\u001b[0;32m--> 147\u001b[0m   vocab_lookup_table \u001b[38;5;241m=\u001b[39m \u001b[43mlookup_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStaticVocabularyTableV1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m      \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_oov_buckets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_key_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(vocab_lookup_table, lookup_ops\u001b[38;5;241m.\u001b[39mLookupInterface):\n\u001b[1;32m    151\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    152\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnable to build a lookup table from \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(vocab_lookup_table))\n",
      "File \u001b[0;32m~/IdeaProjects/debug/operator/jannah-python/lib/python3.11/site-packages/tensorflow/python/trackable/resource.py:103\u001b[0m, in \u001b[0;36m_ResourceMetaclass.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m getter \u001b[38;5;129;01min\u001b[39;00m resource_creator_stack[\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_resource_type()]:\n\u001b[1;32m    101\u001b[0m   previous_getter \u001b[38;5;241m=\u001b[39m _make_getter(getter, previous_getter)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprevious_getter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/IdeaProjects/debug/operator/jannah-python/lib/python3.11/site-packages/tensorflow/python/trackable/resource.py:98\u001b[0m, in \u001b[0;36m_ResourceMetaclass.__call__.<locals>.<lambda>\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     95\u001b[0m   obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m     96\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m---> 98\u001b[0m previous_getter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: \u001b[43mdefault_resource_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m resource_creator_stack \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mget_default_graph()\u001b[38;5;241m.\u001b[39m_resource_creator_stack\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m getter \u001b[38;5;129;01min\u001b[39;00m resource_creator_stack[\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_resource_type()]:\n",
      "File \u001b[0;32m~/IdeaProjects/debug/operator/jannah-python/lib/python3.11/site-packages/tensorflow/python/trackable/resource.py:95\u001b[0m, in \u001b[0;36m_ResourceMetaclass.__call__.<locals>.default_resource_creator\u001b[0;34m(next_creator, *a, **kw)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m next_creator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     94\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m---> 95\u001b[0m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/IdeaProjects/debug/operator/jannah-python/lib/python3.11/site-packages/tensorflow/python/ops/lookup_ops.py:1304\u001b[0m, in \u001b[0;36mStaticVocabularyTable.__init__\u001b[0;34m(self, initializer, num_oov_buckets, lookup_key_dtype, name, experimental_is_anonymous)\u001b[0m\n\u001b[1;32m   1302\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(initializer, trackable_base\u001b[38;5;241m.\u001b[39mTrackable):\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initializer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_track_trackable(initializer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_initializer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1304\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_table \u001b[38;5;241m=\u001b[39m \u001b[43mHashTable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m      \u001b[49m\u001b[43minitializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdefault_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m      \u001b[49m\u001b[43mexperimental_is_anonymous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperimental_is_anonymous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m   name \u001b[38;5;241m=\u001b[39m name \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_table\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/IdeaProjects/debug/operator/jannah-python/lib/python3.11/site-packages/tensorflow/python/trackable/resource.py:103\u001b[0m, in \u001b[0;36m_ResourceMetaclass.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m getter \u001b[38;5;129;01min\u001b[39;00m resource_creator_stack[\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_resource_type()]:\n\u001b[1;32m    101\u001b[0m   previous_getter \u001b[38;5;241m=\u001b[39m _make_getter(getter, previous_getter)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprevious_getter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/IdeaProjects/debug/operator/jannah-python/lib/python3.11/site-packages/tensorflow/python/trackable/resource.py:98\u001b[0m, in \u001b[0;36m_ResourceMetaclass.__call__.<locals>.<lambda>\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     95\u001b[0m   obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m     96\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m---> 98\u001b[0m previous_getter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: \u001b[43mdefault_resource_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m resource_creator_stack \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mget_default_graph()\u001b[38;5;241m.\u001b[39m_resource_creator_stack\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m getter \u001b[38;5;129;01min\u001b[39;00m resource_creator_stack[\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_resource_type()]:\n",
      "File \u001b[0;32m~/IdeaProjects/debug/operator/jannah-python/lib/python3.11/site-packages/tensorflow/python/trackable/resource.py:95\u001b[0m, in \u001b[0;36m_ResourceMetaclass.__call__.<locals>.default_resource_creator\u001b[0;34m(next_creator, *a, **kw)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m next_creator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     94\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m---> 95\u001b[0m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/IdeaProjects/debug/operator/jannah-python/lib/python3.11/site-packages/tensorflow/python/ops/lookup_ops.py:348\u001b[0m, in \u001b[0;36mStaticHashTable.__init__\u001b[0;34m(self, initializer, default_value, name, experimental_is_anonymous)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhash_table\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_table_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mStaticHashTable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdefault_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitializer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_value\u001b[38;5;241m.\u001b[39mget_shape()\n",
      "File \u001b[0;32m~/IdeaProjects/debug/operator/jannah-python/lib/python3.11/site-packages/tensorflow/python/ops/lookup_ops.py:205\u001b[0m, in \u001b[0;36mInitializableLookupTableBase.__init__\u001b[0;34m(self, default_value, initializer)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_op \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize()\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_op \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/IdeaProjects/debug/operator/jannah-python/lib/python3.11/site-packages/tensorflow/python/ops/lookup_ops.py:208\u001b[0m, in \u001b[0;36mInitializableLookupTableBase._initialize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_initialize\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 208\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initializer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/IdeaProjects/debug/operator/jannah-python/lib/python3.11/site-packages/tensorflow/python/ops/lookup_ops.py:772\u001b[0m, in \u001b[0;36mTextFileInitializer.initialize\u001b[0;34m(self, table)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_file_init\u001b[39m\u001b[38;5;124m\"\u001b[39m, (table\u001b[38;5;241m.\u001b[39mresource_handle,)):\n\u001b[1;32m    770\u001b[0m   filename \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\n\u001b[1;32m    771\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename, dtypes\u001b[38;5;241m.\u001b[39mstring, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masset_filepath\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 772\u001b[0m   init_op \u001b[38;5;241m=\u001b[39m \u001b[43mgen_lookup_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_table_from_text_file_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresource_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_key_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_value_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vocab_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_delimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_offset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    776\u001b[0m ops\u001b[38;5;241m.\u001b[39madd_to_collection(ops\u001b[38;5;241m.\u001b[39mGraphKeys\u001b[38;5;241m.\u001b[39mTABLE_INITIALIZERS, init_op)\n\u001b[1;32m    777\u001b[0m \u001b[38;5;66;03m# If the filename tensor is anything other than a string constant (e.g.,\u001b[39;00m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;66;03m# if it is a placeholder) then it does not make sense to track it as an\u001b[39;00m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;66;03m# asset.\u001b[39;00m\n",
      "File \u001b[0;32m~/IdeaProjects/debug/operator/jannah-python/lib/python3.11/site-packages/tensorflow/python/ops/gen_lookup_ops.py:688\u001b[0m, in \u001b[0;36minitialize_table_from_text_file_v2\u001b[0;34m(table_handle, filename, key_index, value_index, vocab_size, delimiter, offset, name)\u001b[0m\n\u001b[1;32m    686\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 688\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m    690\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/IdeaProjects/debug/operator/jannah-python/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:5888\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5886\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5887\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5888\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: {{function_node __wrapped__InitializeTableFromTextFileV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} HashTable has different value for same key. Key 2023-10-03 has 6 and trying to add value 4 [Op:InitializeTableFromTextFileV2] name: "
     ]
    }
   ],
   "source": [
    "bert_tokenizer = text.BertTokenizer(lookup_table, token_out_type=tf.string)\n",
    "bert_tokenizer.tokenize(examples[\"text_a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AiTs3_FHHBlR"
   },
   "outputs": [],
   "source": [
    "bert_tokenizer.tokenize(examples[\"text_b\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cK6DHjio65MV"
   },
   "source": [
    "Text output from [`text.BertTokenizer`](https://tensorflow.org/text/api_docs/python/text/BertTokenizer) allows us see how the text is being tokenized, but the model requires integer IDs. We can set the `token_out_type` param to `tf.int64` to obtain integer IDs (which are the indices into the vocabulary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "odeosiPz58Qu"
   },
   "outputs": [],
   "source": [
    "bert_tokenizer = text.BertTokenizer(lookup_table, token_out_type=tf.int64)\n",
    "segment_a = bert_tokenizer.tokenize(examples[\"text_a\"])\n",
    "segment_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4IP2P4EHQpa"
   },
   "outputs": [],
   "source": [
    "segment_b = bert_tokenizer.tokenize(examples[\"text_b\"])\n",
    "segment_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TU3GJ0jx94fx"
   },
   "source": [
    "[`text.BertTokenizer`](https://tensorflow.org/text/api_docs/python/text/BertTokenizer) returns a `RaggedTensor` with shape `[batch, num_tokens, num_wordpieces]`. Because we don't need the extra `num_tokens` dimensions for our current use case,  we can merge the last two dimensions to obtain a `RaggedTensor` with shape `[batch, num_wordpieces]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fb5vt5dA-Rwf"
   },
   "outputs": [],
   "source": [
    "segment_a = segment_a.merge_dims(-2, -1)\n",
    "segment_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyEW0sjhHoPM"
   },
   "outputs": [],
   "source": [
    "segment_b = segment_b.merge_dims(-2, -1)\n",
    "segment_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9YicLN5UFkz"
   },
   "source": [
    "### Content Trimming\n",
    "\n",
    "The main input to BERT is a concatenation of two sentences. However, BERT requires inputs to be in a fixed-size and shape and we may have content which exceed our budget. \n",
    "\n",
    "We can tackle this by using a [`text.Trimmer`](https://tensorflow.org/text/api_docs/python/text/Trimmer) to trim our content down to a predetermined size (once concatenated along the last axis). There are different `text.Trimmer` types which select content to preserve using different algorithms. [`text.RoundRobinTrimmer`](https://tensorflow.org/text/api_docs/python/text/RoundRobinTrimmer) for example will allocate quota equally for each segment but may trim the ends of sentences. [`text.WaterfallTrimmer`](https://tensorflow.org/text/api_docs/python/text/WaterfallTrimmer) will trim starting from the end of the last sentence.\n",
    "\n",
    "For our example, we will use `RoundRobinTrimmer` which selects items from each segment in a left-to-right manner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLV-1uDgwFnr"
   },
   "outputs": [],
   "source": [
    "trimmer = text.RoundRobinTrimmer(max_seq_length=_MAX_SEQ_LEN)\n",
    "trimmed = trimmer.trim([segment_a, segment_b])\n",
    "trimmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPj7jM9oQ-P3"
   },
   "source": [
    "`trimmed` now contains the segments where the number of elements across a batch is 8 elements (when concatenated along axis=-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3J2AWfmAUio8"
   },
   "source": [
    "### Combining segments\n",
    "\n",
    "Now that we have segments trimmed, we can combine them together to get a single `RaggedTensor`. BERT uses special tokens to indicate the beginning (`[CLS]`) and end of a segment (`[SEP]`). We also need a `RaggedTensor` indicating which items in the combined `Tensor` belong to which segment. We can use [`text.combine_segments()`](https://tensorflow.org/text/api_docs/python/text/combine_segments) to get both of these `Tensor` with special tokens inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-5nMh5pk8x1"
   },
   "outputs": [],
   "source": [
    "segments_combined, segments_ids = text.combine_segments(\n",
    "  trimmed,\n",
    "  start_of_sequence_id=_START_TOKEN, end_of_segment_id=_END_TOKEN)\n",
    "segments_combined, segments_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSKla2OxUOWl"
   },
   "source": [
    "### Masked Language Model Task\n",
    "\n",
    "Now that we have our basic inputs, we can begin to extract the inputs needed for the \"Masked LM and Masking Procedure\" task described in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "\n",
    "The masked language model task has two sub-problems for us to think about: (1) what items to select for masking and (2) what values are they assigned? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkx4w9-3DT0p"
   },
   "source": [
    "#### Item Selection\n",
    "Because we will choose to select items randomly for masking, we will use a [`text.RandomItemSelector`](https://tensorflow.org/text/api_docs/python/text/RandomItemSelector). `RandomItemSelector` randomly selects items in a batch subject to restrictions given (`max_selections_per_batch`, `selection_rate` and `unselectable_ids`) and returns a boolean mask indicating which items were selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94BncqVkVJT2"
   },
   "outputs": [],
   "source": [
    "random_selector = text.RandomItemSelector(\n",
    "    max_selections_per_batch=_MAX_PREDICTIONS_PER_BATCH,\n",
    "    selection_rate=0.2,\n",
    "    unselectable_ids=[_START_TOKEN, _END_TOKEN, _UNK_TOKEN]\n",
    ")\n",
    "selected = random_selector.get_selection_mask(\n",
    "    segments_combined, axis=1)\n",
    "selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4NAHL_GUi-C"
   },
   "source": [
    "#### Choosing the Masked Value\n",
    "\n",
    "The methodology described the original BERT paper for choosing the value for masking is as follows:\n",
    "\n",
    "For `mask_token_rate` of the time, replace the item with the `[MASK]` token:\n",
    "\n",
    "    \"my dog is hairy\" -> \"my dog is [MASK]\"\n",
    " \n",
    "For `random_token_rate` of the time, replace the item with a random word:\n",
    "\n",
    "    \"my dog is hairy\" -> \"my dog is apple\"\n",
    " \n",
    "For `1 - mask_token_rate - random_token_rate` of the time, keep the item\n",
    "unchanged:\n",
    "\n",
    "    \"my dog is hairy\" -> \"my dog is hairy.\"\n",
    "\n",
    "[`text.MaskedValuesChooser`](https://tensorflow.org/text/api_docs/python/text/MaskValuesChooser) encapsulates this logic and can be used for our preprocessing function. Here's an example of what `MaskValuesChooser` returns given a `mask_token_rate` of 80% and default `random_token_rate`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Amk0Lqd5VJ4n"
   },
   "outputs": [],
   "source": [
    "mask_values_chooser = text.MaskValuesChooser(_VOCAB_SIZE, _MASK_TOKEN, 0.8)\n",
    "mask_values_chooser.get_mask_values(segments_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCp1CQcPC6IT"
   },
   "source": [
    "When supplied with a `RaggedTensor` input, `text.MaskValuesChooser` returns a `RaggedTensor` of the same shape with either `_MASK_VALUE` (0), a random ID, or the same unchanged id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYpKg_sLUi1B"
   },
   "source": [
    "#### Generating Inputs for Masked Language Model Task\n",
    "\n",
    "Now that we have a `RandomItemSelector` to help us select items for masking and `text.MaskValuesChooser` to assign the values, we can use [`text.mask_language_model()`](https://tensorflow.org/text/api_docs/python/text/mask_language_model) to assemble all the inputs of this task for our BERT model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q0fqQzXGUrkM"
   },
   "outputs": [],
   "source": [
    "masked_token_ids, masked_pos, masked_lm_ids = text.mask_language_model(\n",
    "  segments_combined,\n",
    "  item_selector=random_selector, mask_values_chooser=mask_values_chooser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJqcbOJ0AYBX"
   },
   "source": [
    "Let's dive deeper and examine the outputs of `mask_language_model()`. The output of `masked_token_ids` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PavYEhmN_tHa"
   },
   "outputs": [],
   "source": [
    "masked_token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0c2wkC9AnUX"
   },
   "source": [
    "Remember that our input is encoded using a vocabulary. If we decode `masked_token_ids` using our vocabulary, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5axqrUOc_0h1"
   },
   "outputs": [],
   "source": [
    "tf.gather(_VOCAB, masked_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8DCOtEAiz_E"
   },
   "source": [
    "Notice that some wordpiece tokens have been replaced with either `[MASK]`, `[RANDOM]` or a different ID value. `masked_pos` output gives us the indices (in the respective batch) of the tokens that have been replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-nc5m5Y_wP_"
   },
   "outputs": [],
   "source": [
    "masked_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fua7ANijN3_"
   },
   "source": [
    "`masked_lm_ids` gives us the original value of the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azzxmO_f_xJp"
   },
   "outputs": [],
   "source": [
    "masked_lm_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bW0rdX9jYh-"
   },
   "source": [
    "We can again decode the IDs here to get human readable values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-RP-paUjUuP"
   },
   "outputs": [],
   "source": [
    "tf.gather(_VOCAB, masked_lm_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-P0PTiCUz2J"
   },
   "source": [
    "### Padding Model Inputs\n",
    "\n",
    "Now that we have all the inputs for our model, the last step in our preprocessing is to package them into fixed 2-dimensional `Tensor`s with padding and also generate a mask `Tensor` indicating the values which are pad values. We can use [`text.pad_model_inputs()`](https://tensorflow.org/text/api_docs/python/text/pad_model_inputs) to help us with this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGE7XuXRwsYF"
   },
   "outputs": [],
   "source": [
    "# Prepare and pad combined segment inputs\n",
    "input_word_ids, input_mask = text.pad_model_inputs(\n",
    "  masked_token_ids, max_seq_length=_MAX_SEQ_LEN)\n",
    "input_type_ids, _ = text.pad_model_inputs(\n",
    "  segments_ids, max_seq_length=_MAX_SEQ_LEN)\n",
    "\n",
    "# Prepare and pad masking task inputs\n",
    "masked_lm_positions, masked_lm_weights = text.pad_model_inputs(\n",
    "  masked_pos, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "masked_lm_ids, _ = text.pad_model_inputs(\n",
    "  masked_lm_ids, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "\n",
    "model_inputs = {\n",
    "    \"input_word_ids\": input_word_ids,\n",
    "    \"input_mask\": input_mask,\n",
    "    \"input_type_ids\": input_type_ids,\n",
    "    \"masked_lm_ids\": masked_lm_ids,\n",
    "    \"masked_lm_positions\": masked_lm_positions,\n",
    "    \"masked_lm_weights\": masked_lm_weights,\n",
    "}\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIWy4nVyT6gf"
   },
   "source": [
    "## Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwCdO1Z5yjS-"
   },
   "source": [
    "Let's review what we have so far and assemble our preprocessing function. Here's what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7jKtbVCYTsIC"
   },
   "outputs": [],
   "source": [
    "def bert_pretrain_preprocess(vocab_table, features):\n",
    "  # Input is a string Tensor of documents, shape [batch, 1].\n",
    "  text_a = features[\"text_a\"]\n",
    "  text_b = features[\"text_b\"]\n",
    "\n",
    "  # Tokenize segments to shape [num_sentences, (num_words)] each.\n",
    "  tokenizer = text.BertTokenizer(\n",
    "      vocab_table,\n",
    "      token_out_type=tf.int64)\n",
    "  segments = [tokenizer.tokenize(text).merge_dims(\n",
    "      1, -1) for text in (text_a, text_b)]\n",
    "\n",
    "  # Truncate inputs to a maximum length.\n",
    "  trimmer = text.RoundRobinTrimmer(max_seq_length=6)\n",
    "  trimmed_segments = trimmer.trim(segments)\n",
    "\n",
    "  # Combine segments, get segment ids and add special tokens.\n",
    "  segments_combined, segment_ids = text.combine_segments(\n",
    "      trimmed_segments,\n",
    "      start_of_sequence_id=_START_TOKEN,\n",
    "      end_of_segment_id=_END_TOKEN)\n",
    "\n",
    "  # Apply dynamic masking task.\n",
    "  masked_input_ids, masked_lm_positions, masked_lm_ids = (\n",
    "      text.mask_language_model(\n",
    "        segments_combined,\n",
    "        random_selector,\n",
    "        mask_values_chooser,\n",
    "      )\n",
    "  )\n",
    "  \n",
    "  # Prepare and pad combined segment inputs\n",
    "  input_word_ids, input_mask = text.pad_model_inputs(\n",
    "    masked_input_ids, max_seq_length=_MAX_SEQ_LEN)\n",
    "  input_type_ids, _ = text.pad_model_inputs(\n",
    "    segment_ids, max_seq_length=_MAX_SEQ_LEN)\n",
    "\n",
    "  # Prepare and pad masking task inputs\n",
    "  masked_lm_positions, masked_lm_weights = text.pad_model_inputs(\n",
    "    masked_lm_positions, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "  masked_lm_ids, _ = text.pad_model_inputs(\n",
    "    masked_lm_ids, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "\n",
    "  model_inputs = {\n",
    "      \"input_word_ids\": input_word_ids,\n",
    "      \"input_mask\": input_mask,\n",
    "      \"input_type_ids\": input_type_ids,\n",
    "      \"masked_lm_ids\": masked_lm_ids,\n",
    "      \"masked_lm_positions\": masked_lm_positions,\n",
    "      \"masked_lm_weights\": masked_lm_weights,\n",
    "  }\n",
    "  return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOAeo97VyfQg"
   },
   "source": [
    "We previously constructed a `tf.data.Dataset` and we can now use our assembled preprocessing function `bert_pretrain_preprocess()` in `Dataset.map()`. This allows us to create an input pipeline for transforming our raw string data into integer inputs and feed directly into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xm4gTLEgjTa3"
   },
   "outputs": [],
   "source": [
    "dataset = (\n",
    "    tf.data.Dataset.from_tensors(examples)\n",
    "    .map(functools.partial(bert_pretrain_preprocess, lookup_table))\n",
    ")\n",
    "\n",
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyiMxeEp0m2O"
   },
   "source": [
    "## Related Tutorials\n",
    "\n",
    "* [Classify text with BERT](https://www.tensorflow.org/text/tutorials/classify_text_with_bert) - A tutorial on how to use a pretrained BERT model to classify text. This is a nice follow up now that you are familiar with how to preprocess the inputs used by the BERT model.\n",
    "\n",
    "* [Tokenizing with TF Text](https://www.tensorflow.org/text/guide/tokenizers) - Tutorial detailing the different types of tokenizers that exist in TF.Text.\n",
    "\n",
    "* [Handling Text with `RaggedTensor`](https://www.tensorflow.org/guide/ragged_tensor) - Detailed guide on how to create, use and manipulate `RaggedTensor`s.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "bert_preprocessing_guide.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
