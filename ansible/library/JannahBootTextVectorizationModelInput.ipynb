{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e69942bf-903b-4bdd-8382-5c87631d1aaa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Learn to Determine, Classify the Score (pass/fail) for a Jannah.io Boot Deployment Using Log Files - TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42220cc9-aee2-4854-9de9-56111604c10a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#papermill_description=Import_Python_Libraries\n",
    "import pathlib\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import collections\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as tf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd0f837d-eb93-4597-8253-abaffefcff5b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'provisioner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/osmanjalloh/working/debug/operator/ansible/library/JannahBootTextVectorizationModelInput.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/osmanjalloh/working/debug/operator/ansible/library/JannahBootTextVectorizationModelInput.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#papermill_description=Determine_Directory_Paths_For_Sorting_Log_Files\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/osmanjalloh/working/debug/operator/ansible/library/JannahBootTextVectorizationModelInput.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m Jannah_Config \u001b[39m=\u001b[39m provisioner[\u001b[39m\"\u001b[39m\u001b[39minventory\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mgroup_vars\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mJannah\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/osmanjalloh/working/debug/operator/ansible/library/JannahBootTextVectorizationModelInput.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m Work_Dir \u001b[39m=\u001b[39m provisioner[\u001b[39m\"\u001b[39m\u001b[39minventory\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mgroup_vars\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mJannah\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mglobal\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mansible\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mworking_dir\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/osmanjalloh/working/debug/operator/ansible/library/JannahBootTextVectorizationModelInput.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m MOLECULE_EPHEMERAL_DIRECTORY \u001b[39m=\u001b[39m provisioner[\u001b[39m\"\u001b[39m\u001b[39menv\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mMOLECULE_EPHEMERAL_DIRECTORY\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'provisioner' is not defined"
     ]
    }
   ],
   "source": [
    "#papermill_description=Determine_Directory_Paths_For_Sorting_Log_Files\n",
    "Jannah_Config = provisioner[\"inventory\"][\"group_vars\"][\"all\"][\"Jannah\"]\n",
    "Work_Dir = provisioner[\"inventory\"][\"group_vars\"][\"all\"][\"Jannah\"]['global']['ansible']['working_dir']\n",
    "MOLECULE_EPHEMERAL_DIRECTORY = provisioner[\"env\"][\"MOLECULE_EPHEMERAL_DIRECTORY\"]\n",
    "Logs_input = f\"{Work_Dir}/logs/TextVectorization\"\n",
    "Logs_train_input = f\"{Logs_input}/train\"\n",
    "Logs_test_input = f\"{Logs_input}/test\"\n",
    "Logs_train_input_pass_deployment = f\"{Logs_train_input}/pass\"\n",
    "Logs_train_input_fail_deployment = f\"{Logs_train_input}/fail\"\n",
    "Logs_test_input_pass_deployment = f\"{Logs_test_input}/pass\"\n",
    "Logs_test_input_fail_deployment = f\"{Logs_test_input}/fail\"\n",
    "sorted_deployment_log_paths = {\n",
    "    \"train\": {\n",
    "                \"pass\": Logs_train_input_pass_deployment,\n",
    "                \"fail\": Logs_train_input_fail_deployment\n",
    "    },\n",
    "    \"test\": {\n",
    "            \"pass\": Logs_test_input_pass_deployment,\n",
    "            \"fail\": Logs_test_input_fail_deployment\n",
    "    }\n",
    "}\n",
    "deploy_to_docker_desktop_before_training = True\n",
    "deploy_to_kind_cluster_before_training = True\n",
    "_ANSIBLE_VERBOSE_LEVEL=\"-vvvvv\"\n",
    "_MODEL_EPOCHS = 3\n",
    "_BATCH_SIZE = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Kind Cluster Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Generate_Log_File_Name\n",
    "current_datetime = datetime.datetime.fromtimestamp(time.time()).isoformat().replace(\":\",\"-\").replace(\".\",\"-\")\n",
    "!mkdir -vp $Logs_input\n",
    "deploy_mode = \"deploy-to-kind-cluster-full-ubuntu-dev-mode\"\n",
    "log_file = f\"{Logs_input}/{deploy_mode}-{current_datetime}.txt\"\n",
    "print(log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=make_deploy_to_kind_cluster_full_ubuntu_dev_mode\n",
    "if deploy_to_kind_cluster_before_training:\n",
    "    !touch $log_file\n",
    "    print(log_file)\n",
    "    !pushd $Work_Dir && ANSIBLE_VERBOSE_LEVEL=$_ANSIBLE_VERBOSE_LEVEL make $deploy_mode >> $log_file && popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=deploy_to_kind_cluster_local_ubuntu_dev_mode\n",
    "deploy_mode = \"deploy-to-kind-cluster-local-ubuntu-dev-mode\"\n",
    "log_file = f\"{Logs_input}/{deploy_mode}-{current_datetime}.txt\"\n",
    "if deploy_to_kind_cluster_before_training:\n",
    "    !touch $log_file\n",
    "    print(log_file)\n",
    "    !pushd $Work_Dir && ANSIBLE_VERBOSE_LEVEL=$_ANSIBLE_VERBOSE_LEVEL make $deploy_mode >> $log_file && popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=deploy_to_kind_cluster_standalone_ubuntu_dev_mode\n",
    "deploy_mode = \"deploy-to-kind-cluster-standalone-ubuntu-dev-mode\"\n",
    "log_file = f\"{Logs_input}/{deploy_mode}-{current_datetime}.txt\"\n",
    "if deploy_to_kind_cluster_before_training:\n",
    "    !touch $log_file\n",
    "    print(log_file)\n",
    "    !pushd $Work_Dir && ANSIBLE_VERBOSE_LEVEL=$_ANSIBLE_VERBOSE_LEVEL make $deploy_mode >> $log_file && popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=jannah_deployment_with_ansible\n",
    "deploy_mode = \"jannah-deployment-with-ansible\"\n",
    "log_file = f\"{Logs_input}/{deploy_mode}-{current_datetime}.txt\"\n",
    "if deploy_to_kind_cluster_before_training:\n",
    "    !touch $log_file\n",
    "    print(log_file)\n",
    "    !pushd $Work_Dir && ANSIBLE_VERBOSE_LEVEL=$_ANSIBLE_VERBOSE_LEVEL make $deploy_mode >> $log_file && popd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Docker Desktop Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context \"docker-desktop\" modified.\n"
     ]
    }
   ],
   "source": [
    "#papermill_description=kubectl_config_set_context_docker_desktop\n",
    "!kubectl config set-context docker-desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=deploy_to_docker_desktop_full_ubuntu_dev_mode\n",
    "deploy_mode = \"deploy-to-docker-desktop-full-ubuntu-dev-mode\"\n",
    "log_file = f\"{Logs_input}/{deploy_mode}-{current_datetime}.txt\"\n",
    "if deploy_to_docker_desktop_before_training:\n",
    "    !touch $log_file\n",
    "    print(log_file)\n",
    "    !pushd $Work_Dir && ANSIBLE_VERBOSE_LEVEL=$_ANSIBLE_VERBOSE_LEVEL make $deploy_mode >> $log_file && popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=deploy_to_docker_desktop_local_ubuntu_dev_mode\n",
    "deploy_mode = \"deploy-to-docker-desktop-local-ubuntu-dev-mode\"\n",
    "log_file = f\"{Logs_input}/{deploy_mode}-{current_datetime}.txt\"\n",
    "if deploy_to_docker_desktop_before_training:\n",
    "    !touch $log_file\n",
    "    print(log_file)\n",
    "    !pushd $Work_Dir && ANSIBLE_VERBOSE_LEVEL=$_ANSIBLE_VERBOSE_LEVEL make $deploy_mode >> $log_file && popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=deploy_to_docker_desktop_standalone_ubuntu_dev_mode\n",
    "deploy_mode = \"deploy-to-docker-desktop-standalone-ubuntu-dev-mode\"\n",
    "log_file = f\"{Logs_input}/{deploy_mode}-{current_datetime}.txt\"\n",
    "if deploy_to_docker_desktop_before_training:\n",
    "    !touch $log_file\n",
    "    print(log_file)\n",
    "    !pushd $Work_Dir && ANSIBLE_VERBOSE_LEVEL=$_ANSIBLE_VERBOSE_LEVEL make $deploy_mode >> $log_file && popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Create_Directories_For_Sorting_Log_Files\n",
    "!mkdir -vp $Logs_train_input_pass_deployment/\n",
    "!mkdir -vp $Logs_train_input_fail_deployment/\n",
    "!mkdir -vp $Logs_test_input_pass_deployment/\n",
    "!mkdir -vp $Logs_test_input_fail_deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Copy_Log_Files_For_Processing\n",
    "!cp -rp ~/jannah-operator/*.log $Logs_input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5745fb9e-1752-4a16-8cb5-afdb9c0201ec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#papermill_description=Glob_Log_Files\n",
    "log_files  = pathlib.Path(Logs_input).glob(\"*.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Note: To increase the difficulty of the classification problem, replace occurrences of the phrases \"All assertions passed\", \"Assertion failed\" from buffer\n",
    "#      and write buffer back to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Determine_Pass_or_Fail_Status_For_Each_Deployment_File\n",
    "_FILE_BUF = \"\"\n",
    "pass_deployments = []\n",
    "failed_deployments = []\n",
    "for _file in sorted((log_files)):\n",
    "    with _file.open() as fd:\n",
    "        _FILE_BUF = fd.read()\n",
    "        if \"All assertions passed\" in _FILE_BUF and \"Assertion failed\" not in _FILE_BUF:\n",
    "            pass_deployments.append(_file)\n",
    "        else:\n",
    "            failed_deployments.append(_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Print_Pass_Deployment_Counts\n",
    "len(pass_deployments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Print_Failed_Deployment_Counts\n",
    "len(failed_deployments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Sort_Pass_Deployments_Into_Train_or_Test_Directories\n",
    "import random\n",
    "for _file in pass_deployments:\n",
    "    train_or_test_group = random.choices(population=['train','test'], weights=[80,20]).pop()\n",
    "    _new_file_name = _file.name.replace(\".log\",\".txt\")\n",
    "    _new_file = f\"{sorted_deployment_log_paths[train_or_test_group]['pass']}/{_new_file_name}\"\n",
    "    _file.rename(_new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Sort_Fail_Deployments_Into_Train_or_Test_Directories\n",
    "import random\n",
    "for _file in failed_deployments:\n",
    "    train_or_test_group = random.choices(population=['train','test'], weights=[80,20]).pop()\n",
    "    _new_file_name = _file.name.replace(\".log\",\".txt\")\n",
    "    _new_file = f\"{sorted_deployment_log_paths[train_or_test_group]['fail']}/{_new_file_name}\"\n",
    "    _file.rename(_new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Text_Dataset_From_Directory\n",
    "train_dir=Logs_train_input\n",
    "batch_size = _BATCH_SIZE\n",
    "#batch_size = 32\n",
    "seed = 80\n",
    "\n",
    "raw_train_ds = utils.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Print_a_Sample_Line\n",
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "  for i in range(_BATCH_SIZE):\n",
    "    print(\"Log Line: \", text_batch.numpy()[i])\n",
    "    print(\"Label:\", label_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Print_the_Labels\n",
    "for i, label in enumerate(raw_train_ds.class_names):\n",
    "  print(\"Label\", i, \"corresponds to\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Create_a_validation_set.\n",
    "raw_val_ds = utils.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Create_a_test_set\n",
    "test_dir = Logs_test_input\n",
    "raw_test_ds = utils.text_dataset_from_directory(\n",
    "    test_dir,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Configure_the_datasets_for_performance\n",
    "raw_train_ds = raw_train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "raw_val_ds = raw_val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "raw_test_ds = raw_test_ds.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First, use the `'binary'` vectorization mode to build a bag-of-words model. Then use the `'int'` mode with a 1D ConvNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Vectorization_binary_mode_to_build_a_bag_of_words_model\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "binary_vectorize_layer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `'int'` mode, in addition to maximum vocabulary size, you need to set an explicit maximum sequence length (`MAX_SEQUENCE_LENGTH`), which will cause the layer to pad or truncate sequences to exactly `output_sequence_length` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Vectorization_int_mode_to_build_a_bag_of_words_model\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "\n",
    "int_vectorize_layer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, call `TextVectorization.adapt` to fit the state of the preprocessing layer to the dataset. This will cause the model to build an index of strings to integers.\n",
    "\n",
    "Note: It's important to only use your training data when calling `TextVectorization.adapt`, as using the test set would leak information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=ANSIBLE_VERBOSE_LEVEL=$_ANSIBLE_VERBOSE_LEVEL make_a_text_only_dataset_without_labels_then_call_TextVectorization_adapt\n",
    "train_text = raw_train_ds.map(lambda text, labels: text)\n",
    "binary_vectorize_layer.adapt(train_text)\n",
    "int_vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the result of using these layers to preprocess data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Retrieve_a_batch_log_files_and_labels_from_the_dataset.\n",
    "text_batch, label_batch = next(iter(raw_train_ds))\n",
    "first_log_file, first_label = text_batch[0], label_batch[0]\n",
    "print(\"Log File\", first_log_file)\n",
    "print(\"Label:\", first_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binary vectorization layer returns a multi-hot vector, with a 1 in the location for each token that was in the input string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Print_binary_vectorized_log_lines.\n",
    "print(\"'binary' vectorized Log Files:\",\n",
    "      list(binary_vectorize_layer(first_log_file).numpy()))\n",
    "\n",
    "plt.plot(binary_vectorize_layer(first_log_file).numpy())\n",
    "plt.xlim(0,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Print_int_vectorized_log_lines.\n",
    "print(\"'int' vectorized Log Files:\",\n",
    "      int_vectorize_layer(first_log_file).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Print_binary_vectorize_layer_int_vectorize_layer\n",
    "print(\"binary_vectorize_layer[0:99] ---> \", binary_vectorize_layer.get_vocabulary()[:99])\n",
    "print(\"int_vectorize_layer[0:99] ---> \", int_vectorize_layer.get_vocabulary()[:99])\n",
    "print(\"binary_vectorize_layer size: {}\".format(len(binary_vectorize_layer.get_vocabulary())))\n",
    "print(\"int_vectorize_layer size: {}\".format(len(int_vectorize_layer.get_vocabulary())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "It's time to create your neural network.\n",
    "\n",
    "For the `'binary'` vectorized data, define a simple bag-of-words linear model, then configure and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Train_the_binary_model\n",
    "binary_model = tf.keras.Sequential([\n",
    "    binary_vectorize_layer,\n",
    "    layers.Dense(4)])\n",
    "\n",
    "binary_model.compile(\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "tf.keras.utils.plot_model(binary_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=load_previously_saved_model_if_any\n",
    "import os\n",
    "if os.path.isdir(\"JannahBootTextVectorizationSavedModel.tf\"): \n",
    "    print(\"read previously saved model\")\n",
    "    loaded = tf.saved_model.load('JannahBootTextVectorizationSavedModel.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Train_the_binary_model_bin_history_fit\n",
    "bin_history = binary_model.fit(\n",
    "    raw_train_ds, validation_data=raw_val_ds, epochs=_MODEL_EPOCHS)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will use the `'int'` vectorized layer to build a 1D ConvNet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=def_create_model\n",
    "def create_model(vocab_size, num_labels, vectorizer=None):\n",
    "  my_layers =[]\n",
    "  if vectorizer is not None:\n",
    "    my_layers = [vectorizer]\n",
    "\n",
    "  my_layers.extend([\n",
    "      layers.Embedding(vocab_size, 64, mask_zero=True),\n",
    "      layers.Dropout(0.5),\n",
    "      layers.Conv1D(64, 5, padding=\"valid\", activation=\"relu\", strides=2),\n",
    "      layers.GlobalMaxPooling1D(),\n",
    "      layers.Dense(num_labels)\n",
    "  ])\n",
    "\n",
    "  model = tf.keras.Sequential(my_layers)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=int_model_create\n",
    "int_model = create_model(vocab_size=VOCAB_SIZE + 1, num_labels=4, vectorizer=int_vectorize_layer)\n",
    "\n",
    "tf.keras.utils.plot_model(int_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=int_model_compile\n",
    "int_model.compile(\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "int_history = int_model.fit(raw_train_ds, validation_data=raw_val_ds, epochs=_MODEL_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=int_model_compile\n",
    "loss = plt.plot(bin_history.epoch, bin_history.history['loss'], label='bin-loss')\n",
    "plt.plot(bin_history.epoch, bin_history.history['val_loss'], '--', color=loss[0].get_color(), label='bin-val_loss')\n",
    "\n",
    "loss = plt.plot(int_history.epoch, int_history.history['loss'], label='int-loss')\n",
    "plt.plot(int_history.epoch, int_history.history['val_loss'], '--', color=loss[0].get_color(), label='int-val_loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('CE/token')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are nearly ready to train your model.\n",
    "\n",
    "As a final preprocessing step, you will apply the `TextVectorization` layers you created earlier to the training, validation, and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=apply_text_vectorization\n",
    "binary_train_ds = raw_train_ds.map(lambda x,y: (binary_vectorize_layer(x), y))\n",
    "binary_val_ds = raw_val_ds.map(lambda x,y: (binary_vectorize_layer(x), y))\n",
    "binary_test_ds = raw_test_ds.map(lambda x,y: (binary_vectorize_layer(x), y))\n",
    "\n",
    "int_train_ds = raw_train_ds.map(lambda x,y: (int_vectorize_layer(x), y))\n",
    "int_val_ds = raw_val_ds.map(lambda x,y: (int_vectorize_layer(x), y))\n",
    "int_test_ds = raw_test_ds.map(lambda x,y: (int_vectorize_layer(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=export_bin_tf\n",
    "binary_model.export('JannahBootTextVectorizationSavedModel.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=saved_model\n",
    "loaded = tf.saved_model.load('JannahBootTextVectorizationSavedModel.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=binary_model_predict_sample_Line\n",
    "for text_batch, label_batch in raw_test_ds.take(1):\n",
    "  for i in range(_BATCH_SIZE):\n",
    "    print(\"Log Line: \", text_batch.numpy()[i])\n",
    "    print(\"Label:\", label_batch.numpy()[i])\n",
    "    binary_model.predict([text_batch.numpy()[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=binary_model_serve_sample_Line\n",
    "for text_batch, label_batch in raw_test_ds.take(1):\n",
    "  for i in range(_BATCH_SIZE):\n",
    "    print(\"Log Line: \", text_batch.numpy()[i])\n",
    "    print(\"Label:\", label_batch.numpy()[i])\n",
    "    loaded.serve(tf.constant([text_batch.numpy()[i]])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn to Determine, Classify the Score (pass/fail) for a Jannah.io Boot Deployment Using Log Files - TextLineDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=train_and_test_files\n",
    "TRAIN_FILE_NAMES = []\n",
    "TEST_FILE_NAMES = []\n",
    "TRAIN_FILE_NAMES.extend(pathlib.Path(Logs_input+\"/train/pass\").glob(\"*.txt\"))\n",
    "TRAIN_FILE_NAMES.extend(pathlib.Path(Logs_input+\"/train/fail\").glob(\"*.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "\n",
    "Previously, with `tf.keras.utils.text_dataset_from_directory` all contents of a file were treated as a single example. Here, you will use `tf.data.TextLineDataset`, which is designed to create a `tf.data.Dataset` from a text file where each example is a line of text from the original file. `TextLineDataset` is useful for text data that is primarily line-based (for example, poetry or error logs).\n",
    "\n",
    "Iterate through these files, loading each one into its own dataset. Each example needs to be individually labeled, so use `Dataset.map` to apply a labeler function to each one. This will iterate over every example in the dataset, returning (`example, label`) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=define_labeler\n",
    "def labeler(example, index):\n",
    "  return example, tf.cast(index, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=read_text_line_dataset\n",
    "labeled_data_sets = []\n",
    "\n",
    "for i, file_name in enumerate(TRAIN_FILE_NAMES):\n",
    "  lines_dataset = tf.data.TextLineDataset(file_name)\n",
    "  labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))\n",
    "  labeled_data_sets.append(labeled_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you'll combine these labeled datasets into a single dataset using `Dataset.concatenate`, and shuffle it with `Dataset.shuffle`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=set_buffer_size\n",
    "BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "VALIDATION_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=all_labeled_data_concatenate\n",
    "all_labeled_data = labeled_data_sets[0]\n",
    "for labeled_dataset in labeled_data_sets[1:]:\n",
    "  all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\n",
    "\n",
    "all_labeled_data = all_labeled_data.shuffle(\n",
    "    BUFFER_SIZE, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out a few examples as before. The dataset hasn't been batched yet, hence each entry in `all_labeled_data` corresponds to one data point:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, label in all_labeled_data.take(10):\n",
    "  print(\"Log Line: \", text.numpy())\n",
    "  print(\"Label:\", label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset for training\n",
    "\n",
    "Instead of using `tf.keras.layers.TextVectorization` to preprocess the text dataset, you will now use the lower-level TensorFlow Text APIs to standardize and tokenize the data, build a vocabulary and use `tf.lookup.StaticVocabularyTable` to map tokens to integers to feed to the model. (Learn more about [TensorFlow Text](https://www.tensorflow.org/text)).\n",
    "\n",
    "Define a function to convert the text to lower-case and tokenize it:\n",
    "\n",
    "- TensorFlow Text provides various tokenizers. In this example, you will use the `text.UnicodeScriptTokenizer` to tokenize the dataset.\n",
    "- You will use `Dataset.map` to apply the tokenization to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=define_jannahbootlogtokenizer\n",
    "class JannahBootLogTokenizer(tf.keras.layers.Layer):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.tokenizer = tf_text.UnicodeScriptTokenizer()\n",
    "\n",
    "  def call(self, text):\n",
    "    lower_case = tf_text.case_fold_utf8(text)\n",
    "    result = self.tokenizer.tokenize(lower_case)\n",
    "    # If you pass a batch of strings, it will return a RaggedTensor.\n",
    "    if isinstance(result, tf.RaggedTensor):\n",
    "      # Convert to dense 0-padded.\n",
    "      result = result.to_tensor()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=instantiate_jannahbootlogtokenizer\n",
    "tokenizer = JannahBootLogTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=tokenized_ds\n",
    "tokenized_ds = all_labeled_data.map(lambda text, label: (tokenizer(text), label))\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can iterate over the dataset and print out a few tokenized examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=print_tokens\n",
    "for tokens, label in tokenized_ds.take(1):\n",
    "  break\n",
    "\n",
    "print(tokens)\n",
    "print()\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will build a vocabulary by sorting tokens by frequency and keeping the top `VOCAB_SIZE` tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=build_vocabulary \n",
    "import collections\n",
    "tokenized_ds = tokenized_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "vocab_count = collections.Counter()\n",
    "for toks, labels in tokenized_ds.ragged_batch(1000):\n",
    "  toks = tf.reshape(toks, [-1])\n",
    "  for tok in toks.numpy():\n",
    "    vocab_count[tok] += 1\n",
    "\n",
    "vocab = [tok for tok, count in vocab_count.most_common(VOCAB_SIZE)]\n",
    "\n",
    "print(\"First five vocab entries:\", vocab[:5])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the tokens into integers, use the `vocab` set to create a `tf.lookup.StaticVocabularyTable`. You will map tokens to integers with `0` reserved for padding, and `n+1` reserved to denote an out-of-vocabulary (OOV) token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=define_vocabulary_table\n",
    "class JannahBootLogsVocabTable(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab):\n",
    "    super().__init__()\n",
    "    self.keys = [''] + vocab\n",
    "    self.values = range(len(self.keys))\n",
    "\n",
    "    self.init = tf.lookup.KeyValueTensorInitializer(\n",
    "        self.keys, self.values, key_dtype=tf.string, value_dtype=tf.int64)\n",
    "\n",
    "    num_oov_buckets = 1\n",
    "\n",
    "    self.table = tf.lookup.StaticVocabularyTable(self.init, num_oov_buckets)\n",
    "\n",
    "  def call(self, x):\n",
    "    result = self.table.lookup(x)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it on a dummy vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=instantiate_vocabulary_table_with_dummy_data\n",
    "vocab_table = JannahBootLogsVocabTable(['a','b','c'])\n",
    "vocab_table(tf.constant([''] + list('abcdefghi')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create one for the real vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=instantiate_vocabulary_table_with_log_data\n",
    "vocab_table = JannahBootLogsVocabTable(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, define a layer to standardize, tokenize and vectorize the dataset using the tokenizer and lookup table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=define_layer_to_standardize_tokenize_and_vectorize_the_dataset\n",
    "preprocess_text = tf.keras.Sequential([\n",
    "    tokenizer,\n",
    "    vocab_table\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try this on a single example to print the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=define_layer_to_standardize_tokenize_and_vectorize_the_dataset\n",
    "example_text, example_label = next(iter(all_labeled_data))\n",
    "print(\"Log Line: \", example_text.numpy())\n",
    "vectorized_text = preprocess_text(example_text)\n",
    "print(\"Vectorized log line: \", vectorized_text.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a dataset pipeline that will process the text on the fly using `Dataset.map`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=print_all_encoded_data\n",
    "all_encoded_data = all_labeled_data.map(lambda text, labels:(preprocess_text(text), labels))\n",
    "\n",
    "for ids, label in all_encoded_data.take(1):\n",
    "  break\n",
    "\n",
    "print(\"Ids: \", ids.numpy())\n",
    "print(\"Label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Keras `TextVectorization` layer also batches and pads the vectorized data. Padding is required because the examples inside of a batch need to be the same size and shape, but the examples in these datasets are not all the same size—each line of text has a different number of words.\n",
    "\n",
    "`tf.data.Dataset` supports splitting and padded-batching datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=split_the_dataset_into_training_and_test_sets\n",
    "train_data = all_encoded_data.skip(VALIDATION_SIZE).shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
    "validation_data = all_encoded_data.take(VALIDATION_SIZE).padded_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `validation_data` and `train_data` are not collections of (`example, label`) pairs, but collections of batches. Each batch is a pair of (*many examples*, *many labels*) represented as arrays.\n",
    "\n",
    "To illustrate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=print_text_batch_label_batch\n",
    "sample_text, sample_labels = next(iter(validation_data))\n",
    "print(\"Text batch shape: \", sample_text.shape)\n",
    "print(\"Label batch shape: \", sample_labels.shape)\n",
    "print(\"First text example: \", sample_text[0])\n",
    "print(\"First label example: \", sample_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the datasets for better performance as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#papermill_description=autotune_train_data_validation_data\n",
    "train_data = train_data.prefetch(tf.data.AUTOTUNE)\n",
    "validation_data = validation_data.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "You can train a model on this dataset as before:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this text vectorization adds `0` for padding and `n+1` for out-of-vocabulary (OOV) tokens, the vocabulary size has increased by two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=create_and_compile_model\n",
    "model = create_model(vocab_size=VOCAB_SIZE+2, num_labels=300)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=plot_model\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=fit_model\n",
    "history = model.fit(train_data, validation_data=validation_data, epochs=_MODEL_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=evaluate_model\n",
    "metrics = model.evaluate(validation_data, return_dict=True)\n",
    "\n",
    "print(\"Loss: \", metrics['loss'])\n",
    "print(\"Accuracy: {:2.2%}\".format(metrics['accuracy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ANSIBLE_VERBOSE_LEVEL=$_ANSIBLE_VERBOSE_LEVEL make the model capable of taking raw strings as input, pack both the text processor and the model into a `keras.Sequential`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=sequential_model\n",
    "export_model = tf.keras.Sequential([\n",
    "    preprocess_text,\n",
    "    model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=sequential_model_compile\n",
    "export_model.compile(\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model you can run directly on batches of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=create_a_test_dataset_of_raw_strings.\n",
    "test_ds = all_labeled_data.take(VALIDATION_SIZE).batch(BATCH_SIZE)\n",
    "test_ds = test_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=sequential_model\n",
    "loss, accuracy = export_model.evaluate(test_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: {:2.2%}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `saved_model.save` to export it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=sequential_model_save\n",
    "tf.saved_model.save(export_model, 'export.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=sequential_model_export\n",
    "loaded = tf.saved_model.load('export.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=sequential_model_export\n",
    "export_model(tf.constant(['TASK [Kubeflow install (standalone deployment) docker-desktop cluster - wait for crd/applications.app.k8s.io] ***'])).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=sequential_model_export\n",
    "loaded(tf.constant(['TASK [Kubeflow install (standalone deployment) docker-desktop cluster - wait for crd/applications.app.k8s.io] ***'])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss and accuracy for the model on encoded validation set and the exported model on the raw validation set are the same, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    \"TASK [Git checkout https://github.com/jannahio/frontend.git] *******************\",  # Label: 1\n",
    "    \"TASK [Create images/ubuntu directory] ******************************************\",  # Label: 2\n",
    "    \"TASK [Import cleanup prepared build image tasks - streamos middleware and frontend] ***\",  # Label: 0\n",
    "]\n",
    "\n",
    "predicted_scores = export_model.predict(inputs)\n",
    "predicted_labels = tf.math.argmax(predicted_scores, axis=1)\n",
    "\n",
    "for input, label in zip(inputs, predicted_labels):\n",
    "  print(\"Log Line: \", input)\n",
    "  print(\"Predicted label: \", label.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
