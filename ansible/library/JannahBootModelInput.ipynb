{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e69942bf-903b-4bdd-8382-5c87631d1aaa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Learn to Determine, Classify the Score (pass/fail) for a Jannah.io Boot Deployment Using Log Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42220cc9-aee2-4854-9de9-56111604c10a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#papermill_description=Import_Python_Libraries\n",
    "import pathlib\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as tf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0f837d-eb93-4597-8253-abaffefcff5b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#papermill_description=Determine_Directory_Paths_For_Sorting_Log_Files\n",
    "Jannah_Config = provisioner[\"inventory\"][\"group_vars\"][\"all\"][\"Jannah\"]\n",
    "Work_Dir = provisioner[\"inventory\"][\"group_vars\"][\"all\"][\"Jannah\"]['global']['ansible']['working_dir']\n",
    "MOLECULE_EPHEMERAL_DIRECTORY = provisioner[\"env\"][\"MOLECULE_EPHEMERAL_DIRECTORY\"]\n",
    "Logs_input = f\"{MOLECULE_EPHEMERAL_DIRECTORY}/logs\"\n",
    "Logs_train_input = f\"{Logs_input}/train\"\n",
    "Logs_test_input = f\"{Logs_input}/test\"\n",
    "Logs_train_input_pass_deployment = f\"{Logs_train_input}/pass\"\n",
    "Logs_train_input_fail_deployment = f\"{Logs_train_input}/fail\"\n",
    "Logs_test_input_pass_deployment = f\"{Logs_test_input}/pass\"\n",
    "Logs_test_input_fail_deployment = f\"{Logs_test_input}/fail\"\n",
    "sorted_deployment_log_paths = {\n",
    "    \"train\": {\n",
    "                \"pass\": Logs_train_input_pass_deployment,\n",
    "                \"fail\": Logs_train_input_fail_deployment\n",
    "    },\n",
    "    \"test\": {\n",
    "            \"pass\": Logs_test_input_pass_deployment,\n",
    "            \"fail\": Logs_test_input_fail_deployment\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Kind Cluster Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Generate_Log_File_Name\n",
    "current_datetime = datetime.datetime.fromtimestamp(time.time()).isoformat().replace(\":\",\"-\").replace(\".\",\"-\")\n",
    "# Logs_input=\"/Users/osmanjalloh/working/debug/operator/tmp/EPHEMERAL/logs\"\n",
    "# Work_Dir = \"/Users/osmanjalloh/working/debug/operator\"\n",
    "deploy_mode = \"deploy-to-kind-cluster-full-ubuntu-dev-mode\"\n",
    "log_file = f\"{Logs_input}/{deploy_mode}-{current_datetime}.txt\"\n",
    "print(dir())\n",
    "print(log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=make_deploy_to_kind_cluster_full_ubuntu_dev_mode\n",
    "!pushd $Work_Dir && make $deploy_mode >> $log_file && popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=deploy_to_kind_cluster_local_ubuntu_dev_mode\n",
    "deploy_mode = \"deploy-to-kind-cluster-local-ubuntu-dev-mode\"\n",
    "log_file = f\"{Logs_input}/{deploy_mode}-{current_datetime}.txt\"\n",
    "print(log_file)\n",
    "!pushd $Work_Dir && make $deploy_mode >> $log_file && popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=deploy_to_kind_cluster_standalone_ubuntu_dev_mode\n",
    "deploy_mode = \"deploy-to-kind-cluster-standalone-ubuntu-dev-mode\"\n",
    "log_file = f\"{Logs_input}/{deploy_mode}-{current_datetime}.txt\"\n",
    "print(log_file)\n",
    "!pushd $Work_Dir && make $deploy_mode >> $log_file && popd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Docker Desktop Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=deploy_to_docker_desktop_full_ubuntu_dev_mode\n",
    "deploy_mode = \"deploy-to-docker-desktop-full-ubuntu-dev-mode\"\n",
    "log_file = f\"{Logs_input}/{deploy_mode}-{current_datetime}.txt\"\n",
    "print(log_file)\n",
    "!pushd $Work_Dir && make $deploy_mode >> $log_file && popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=deploy_to_docker_desktop_local_ubuntu_dev_mode\n",
    "deploy_mode = \"deploy-to-docker-desktop-local-ubuntu-dev-mode\"\n",
    "log_file = f\"{Logs_input}/{deploy_mode}-{current_datetime}.txt\"\n",
    "print(log_file)\n",
    "!pushd $Work_Dir && make $deploy_mode >> $log_file && popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=deploy_to_docker_desktop_local_ubuntu_dev_mode\n",
    "deploy_mode = \"deploy-to-docker-desktop-local-ubuntu-dev-mode\"\n",
    "log_file = f\"{Logs_input}/{deploy_mode}-{current_datetime}.txt\"\n",
    "print(log_file)\n",
    "!pushd $Work_Dir && make deploy_mode >> $log_file && popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=deploy_to_docker_desktop_standalone_ubuntu_dev_mode\n",
    "deploy_mode = \"deploy-to-docker-desktop-standalone-ubuntu-dev-mode\"\n",
    "log_file = f\"{Logs_input}/{deploy_mode}-{current_datetime}.txt\"\n",
    "print(log_file)\n",
    "!pushd $Work_Dir && make $deploy_mode >> $log_file && popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=jannah_deployment_with_ansible\n",
    "deploy_mode = \"jannah-deployment-with-ansible\"\n",
    "log_file = f\"{Logs_input}/{deploy_mode}-{current_datetime}.txt\"\n",
    "print(log_file)\n",
    "!pushd $Work_Dir && make $deploy_mode >> $log_file && popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Create_Directories_For_Sorting_Log_Files\n",
    "!mkdir -vp $Logs_train_input_pass_deployment/\n",
    "!mkdir -vp $Logs_train_input_fail_deployment/\n",
    "!mkdir -vp $Logs_test_input_pass_deployment/\n",
    "!mkdir -vp $Logs_test_input_fail_deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Copy_Log_Files_For_Processing\n",
    "!cp -rp ~/jannah-operator/*.log $Logs_input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5745fb9e-1752-4a16-8cb5-afdb9c0201ec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#papermill_description=Glob_Log_Files\n",
    "log_files  = pathlib.Path(Logs_input).glob(\"*.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Note: To increase the difficulty of the classification problem, th replaced occurrences of the phrases \"All assertions passed\", \"Assertion failed\" from buffer\n",
    "#      and write buffer back to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Determine_Pass_or_Fail_Status_For_Each_Deployment_File\n",
    "_FILE_BUF = \"\"\n",
    "pass_deployments = []\n",
    "failed_deployments = []\n",
    "for _file in sorted((log_files)):\n",
    "    with _file.open() as fd:\n",
    "        _FILE_BUF = fd.read()\n",
    "        if \"All assertions passed\" in _FILE_BUF and \"Assertion failed\" not in _FILE_BUF:\n",
    "            pass_deployments.append(_file)\n",
    "        else:\n",
    "            failed_deployments.append(_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Print_Pass_Deployment_Counts\n",
    "len(pass_deployments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Print_Failed_Deployment_Counts\n",
    "len(failed_deployments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Sort_Pass_Deployments_Into_Train_or_Test_Directories\n",
    "import random\n",
    "for _file in pass_deployments:\n",
    "    train_or_test_group = random.choices(population=['train','test'], weights=[80,20]).pop()\n",
    "    _new_file_name = _file.name.replace(\".log\",\".txt\")\n",
    "    _new_file = f\"{sorted_deployment_log_paths[train_or_test_group]['pass']}/{_new_file_name}\"\n",
    "    _file.rename(_new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Sort_Fail_Deployments_Into_Train_or_Test_Directories\n",
    "import random\n",
    "for _file in failed_deployments:\n",
    "    train_or_test_group = random.choices(population=['train','test'], weights=[80,20]).pop()\n",
    "    _new_file_name = _file.name.replace(\".log\",\".txt\")\n",
    "    _new_file = f\"{sorted_deployment_log_paths[train_or_test_group]['fail']}/{_new_file_name}\"\n",
    "    _file.rename(_new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Text_Dataset_From_Directory\n",
    "train_dir=Logs_train_input\n",
    "batch_size = 32\n",
    "seed = 80\n",
    "\n",
    "raw_train_ds = utils.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Print_a_Sample_Line\n",
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "  for i in range(10):\n",
    "    print(\"Log Line: \", text_batch.numpy()[i])\n",
    "    print(\"Label:\", label_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Print_the_Labels\n",
    "for i, label in enumerate(raw_train_ds.class_names):\n",
    "  print(\"Label\", i, \"corresponds to\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Create_a_validation_set.\n",
    "raw_val_ds = utils.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Create_a_test_set\n",
    "test_dir = Logs_test_input\n",
    "raw_test_ds = utils.text_dataset_from_directory(\n",
    "    test_dir,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Configure_the_datasets_for_performance\n",
    "raw_train_ds = raw_train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "raw_val_ds = raw_val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "raw_test_ds = raw_test_ds.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First, use the `'binary'` vectorization mode to build a bag-of-words model. Then use the `'int'` mode with a 1D ConvNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Vectorization_mode_to_build_a_bag_of_words_model\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "binary_vectorize_layer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `'int'` mode, in addition to maximum vocabulary size, you need to set an explicit maximum sequence length (`MAX_SEQUENCE_LENGTH`), which will cause the layer to pad or truncate sequences to exactly `output_sequence_length` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Vectorization_mode_to_build_a_bag_of_words_model\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "\n",
    "int_vectorize_layer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, call `TextVectorization.adapt` to fit the state of the preprocessing layer to the dataset. This will cause the model to build an index of strings to integers.\n",
    "\n",
    "Note: It's important to only use your training data when calling `TextVectorization.adapt`, as using the test set would leak information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Make_a_text_only_dataset_without_labels_then_call_TextVectorization_adapt\n",
    "train_text = raw_train_ds.map(lambda text, labels: text)\n",
    "binary_vectorize_layer.adapt(train_text)\n",
    "int_vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the result of using these layers to preprocess data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Retrieve_a_batch__of_32_log_lines_and_labels_from_the_dataset.\n",
    "text_batch, label_batch = next(iter(raw_train_ds))\n",
    "first_log_line, first_label = text_batch[0], label_batch[0]\n",
    "print(\"Log Line:\", first_log_line)\n",
    "print(\"Label:\", first_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binary vectorization layer returns a multi-hot vector, with a 1 in the location for each token that was in the input string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Print_binary_vectorized_log_lines.\n",
    "print(\"'binary' vectorized Log lines:\",\n",
    "      list(binary_vectorize_layer(first_log_line).numpy()))\n",
    "\n",
    "plt.plot(binary_vectorize_layer(first_log_line).numpy())\n",
    "plt.xlim(0,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Print_int_vectorized_log_lines.\n",
    "print(\"'int' vectorized question:\",\n",
    "      int_vectorize_layer(first_log_line).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Print_binary_vectorize_layer_int_vectorize_layer\n",
    "print(\"binary_vectorize_layer[0:3] ---> \", binary_vectorize_layer.get_vocabulary()[:3])\n",
    "print(\"int_vectorize_layer[0:3] ---> \", int_vectorize_layer.get_vocabulary()[:3])\n",
    "print(\"binary_vectorize_layer size: {}\".format(len(binary_vectorize_layer.get_vocabulary())))\n",
    "print(\"int_vectorize_layer size: {}\".format(len(int_vectorize_layer.get_vocabulary())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "It's time to create your neural network.\n",
    "\n",
    "For the `'binary'` vectorized data, define a simple bag-of-words linear model, then configure and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Train_the_binary_model\n",
    "binary_model = tf.keras.Sequential([\n",
    "    binary_vectorize_layer,\n",
    "    layers.Dense(4)])\n",
    "\n",
    "binary_model.compile(\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "tf.keras.utils.plot_model(binary_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Train_the_binary_model_bin_history_fit\n",
    "bin_history = binary_model.fit(\n",
    "    raw_train_ds, validation_data=raw_val_ds, epochs=18)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will use the `'int'` vectorized layer to build a 1D ConvNet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=def_create_model\n",
    "def create_model(vocab_size, num_labels, vectorizer=None):\n",
    "  my_layers =[]\n",
    "  if vectorizer is not None:\n",
    "    my_layers = [vectorizer]\n",
    "\n",
    "  my_layers.extend([\n",
    "      layers.Embedding(vocab_size, 64, mask_zero=True),\n",
    "      layers.Dropout(0.5),\n",
    "      layers.Conv1D(64, 5, padding=\"valid\", activation=\"relu\", strides=2),\n",
    "      layers.GlobalMaxPooling1D(),\n",
    "      layers.Dense(num_labels)\n",
    "  ])\n",
    "\n",
    "  model = tf.keras.Sequential(my_layers)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=int_model_create\n",
    "int_model = create_model(vocab_size=VOCAB_SIZE + 1, num_labels=4, vectorizer=int_vectorize_layer)\n",
    "\n",
    "tf.keras.utils.plot_model(int_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=int_model_compile\n",
    "int_model.compile(\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "int_history = int_model.fit(raw_train_ds, validation_data=raw_val_ds, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=int_model_compile\n",
    "loss = plt.plot(bin_history.epoch, bin_history.history['loss'], label='bin-loss')\n",
    "plt.plot(bin_history.epoch, bin_history.history['val_loss'], '--', color=loss[0].get_color(), label='bin-val_loss')\n",
    "\n",
    "loss = plt.plot(int_history.epoch, int_history.history['loss'], label='int-loss')\n",
    "plt.plot(int_history.epoch, int_history.history['val_loss'], '--', color=loss[0].get_color(), label='int-val_loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('CE/token')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are nearly ready to train your model.\n",
    "\n",
    "As a final preprocessing step, you will apply the `TextVectorization` layers you created earlier to the training, validation, and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=int_model_compile\n",
    "binary_train_ds = raw_train_ds.map(lambda x,y: (binary_vectorize_layer(x), y))\n",
    "binary_val_ds = raw_val_ds.map(lambda x,y: (binary_vectorize_layer(x), y))\n",
    "binary_test_ds = raw_test_ds.map(lambda x,y: (binary_vectorize_layer(x), y))\n",
    "\n",
    "int_train_ds = raw_train_ds.map(lambda x,y: (int_vectorize_layer(x), y))\n",
    "int_val_ds = raw_val_ds.map(lambda x,y: (int_vectorize_layer(x), y))\n",
    "int_test_ds = raw_test_ds.map(lambda x,y: (int_vectorize_layer(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=export_bin_tf\n",
    "binary_model.export('bin.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=saved_model\n",
    "loaded = tf.saved_model.load('bin.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=predict\n",
    "binary_model.predict(['2023-10-20 16:32:43,374 p=11445 u=osmanjalloh n=ansible | localhost                  : ok=21   changed=4    unreachable=0    failed=0    skipped=3    rescued=0    ignored=0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=serve\n",
    "loaded.serve(tf.constant(['localhost                  : ok=19   changed=3    unreachable=0    failed=0    skipped=9    rescued=0    ignored=0'])).numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
