{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e69942bf-903b-4bdd-8382-5c87631d1aaa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Learn to Determine, Classify the Score (pass/fail) for a Jannah.io Boot Deployment Using Log Files - TextLineDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42220cc9-aee2-4854-9de9-56111604c10a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#papermill_description=Import_Python_Libraries\n",
    "import pathlib\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import collections\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as tf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0f837d-eb93-4597-8253-abaffefcff5b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#papermill_description=Determine_Directory_Paths_For_Sorting_Log_Files\n",
    "Jannah_Config = provisioner[\"inventory\"][\"group_vars\"][\"all\"][\"Jannah\"]\n",
    "Work_Dir = provisioner[\"inventory\"][\"group_vars\"][\"all\"][\"Jannah\"]['global']['ansible']['working_dir']\n",
    "MOLECULE_EPHEMERAL_DIRECTORY = provisioner[\"env\"][\"MOLECULE_EPHEMERAL_DIRECTORY\"]\n",
    "Logs_input = f\"{Work_Dir}/logs/TextLineDataset\"\n",
    "Logs_train_input = f\"{Logs_input}/train\"\n",
    "Logs_test_input = f\"{Logs_input}/test\"\n",
    "Logs_train_input_pass_deployment = f\"{Logs_train_input}/pass\"\n",
    "Logs_train_input_fail_deployment = f\"{Logs_train_input}/fail\"\n",
    "Logs_test_input_pass_deployment = f\"{Logs_test_input}/pass\"\n",
    "Logs_test_input_fail_deployment = f\"{Logs_test_input}/fail\"\n",
    "sorted_deployment_log_paths = {\n",
    "    \"train\": {\n",
    "                \"pass\": Logs_train_input_pass_deployment,\n",
    "                \"fail\": Logs_train_input_fail_deployment\n",
    "    },\n",
    "    \"test\": {\n",
    "            \"pass\": Logs_test_input_pass_deployment,\n",
    "            \"fail\": Logs_test_input_fail_deployment\n",
    "    }\n",
    "}\n",
    "deploy_to_docker_desktop_before_training = True\n",
    "deploy_to_kind_cluster_before_training = True\n",
    "_ANSIBLE_VERBOSE_LEVEL=\"-vvvvv\"\n",
    "_MODEL_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Kind Cluster Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Generate_Log_File_Name\n",
    "current_datetime = datetime.datetime.fromtimestamp(time.time()).isoformat().replace(\":\",\"-\").replace(\".\",\"-\")\n",
    "# Logs_input=\"/Users/osmanjalloh/working/debug/operator/tmp/EPHEMERAL/logs\"\n",
    "# Work_Dir = \"/Users/osmanjalloh/working/debug/operator\"\n",
    "!mkdir -vp $Logs_input\n",
    "deploy_mode = \"deploy-to-kind-cluster-full-ubuntu-dev-mode\"\n",
    "log_file = f\"{Logs_input}/{deploy_mode}-{current_datetime}.txt\"\n",
    "print(log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=make_deploy_to_kind_cluster_full_ubuntu_dev_mode\n",
    "if deploy_to_kind_cluster_before_training:\n",
    "    !touch $log_file\n",
    "    print(log_file)\n",
    "    !pushd $Work_Dir && ANSIBLE_VERBOSE_LEVEL=$_ANSIBLE_VERBOSE_LEVEL make $deploy_mode >> $log_file && popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=deploy_to_kind_cluster_local_ubuntu_dev_mode\n",
    "deploy_mode = \"deploy-to-kind-cluster-local-ubuntu-dev-mode\"\n",
    "log_file = f\"{Logs_input}/{deploy_mode}-{current_datetime}.txt\"\n",
    "if deploy_to_kind_cluster_before_training:\n",
    "    !touch $log_file\n",
    "    print(log_file)\n",
    "    !pushd $Work_Dir && ANSIBLE_VERBOSE_LEVEL=$_ANSIBLE_VERBOSE_LEVEL make $deploy_mode >> $log_file && popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=deploy_to_kind_cluster_standalone_ubuntu_dev_mode\n",
    "deploy_mode = \"deploy-to-kind-cluster-standalone-ubuntu-dev-mode\"\n",
    "log_file = f\"{Logs_input}/{deploy_mode}-{current_datetime}.txt\"\n",
    "if deploy_to_kind_cluster_before_training:\n",
    "    !touch $log_file\n",
    "    print(log_file)\n",
    "    !pushd $Work_Dir && ANSIBLE_VERBOSE_LEVEL=$_ANSIBLE_VERBOSE_LEVEL make $deploy_mode >> $log_file && popd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Docker Desktop Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=kubectl_config_set_context_docker_desktop\n",
    "!kubectl config set-context docker-desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=deploy_to_docker_desktop_full_ubuntu_dev_mode\n",
    "deploy_mode = \"deploy-to-docker-desktop-full-ubuntu-dev-mode\"\n",
    "log_file = f\"{Logs_input}/{deploy_mode}-{current_datetime}.txt\"\n",
    "if deploy_to_docker_desktop_before_training:\n",
    "    !touch $log_file\n",
    "    print(log_file)\n",
    "    !pushd $Work_Dir && ANSIBLE_VERBOSE_LEVEL=$_ANSIBLE_VERBOSE_LEVEL make $deploy_mode >> $log_file && popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=deploy_to_docker_desktop_local_ubuntu_dev_mode\n",
    "deploy_mode = \"deploy-to-docker-desktop-local-ubuntu-dev-mode\"\n",
    "log_file = f\"{Logs_input}/{deploy_mode}-{current_datetime}.txt\"\n",
    "if deploy_to_docker_desktop_before_training:\n",
    "    !touch $log_file\n",
    "    print(log_file)\n",
    "    !pushd $Work_Dir && ANSIBLE_VERBOSE_LEVEL=$_ANSIBLE_VERBOSE_LEVEL make $deploy_mode >> $log_file && popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=deploy_to_docker_desktop_local_ubuntu_dev_mode\n",
    "deploy_mode = \"deploy-to-docker-desktop-local-ubuntu-dev-mode\"\n",
    "log_file = f\"{Logs_input}/{deploy_mode}-{current_datetime}.txt\"\n",
    "if deploy_to_docker_desktop_before_training:\n",
    "    !touch $log_file\n",
    "    print(log_file)\n",
    "    !pushd $Work_Dir && ANSIBLE_VERBOSE_LEVEL=$_ANSIBLE_VERBOSE_LEVEL make $deploy_mode >> $log_file && popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=deploy_to_docker_desktop_standalone_ubuntu_dev_mode\n",
    "deploy_mode = \"deploy-to-docker-desktop-standalone-ubuntu-dev-mode\"\n",
    "log_file = f\"{Logs_input}/{deploy_mode}-{current_datetime}.txt\"\n",
    "if deploy_to_docker_desktop_before_training:\n",
    "    !touch $log_file\n",
    "    print(log_file)\n",
    "    !pushd $Work_Dir && ANSIBLE_VERBOSE_LEVEL=$_ANSIBLE_VERBOSE_LEVEL make $deploy_mode >> $log_file && popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=jannah_deployment_with_ansible\n",
    "deploy_mode = \"jannah-deployment-with-ansible\"\n",
    "log_file = f\"{Logs_input}/{deploy_mode}-{current_datetime}.txt\"\n",
    "if deploy_to_kind_cluster_before_training:\n",
    "    !touch $log_file\n",
    "    print(log_file)\n",
    "    !pushd $Work_Dir && ANSIBLE_VERBOSE_LEVEL=$_ANSIBLE_VERBOSE_LEVEL make $deploy_mode >> $log_file && popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Create_Directories_For_Sorting_Log_Files\n",
    "!mkdir -vp $Logs_train_input_pass_deployment/\n",
    "!mkdir -vp $Logs_train_input_fail_deployment/\n",
    "!mkdir -vp $Logs_test_input_pass_deployment/\n",
    "!mkdir -vp $Logs_test_input_fail_deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Copy_Log_Files_For_Processing\n",
    "!cp -rp ~/jannah-operator/*.log $Logs_input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5745fb9e-1752-4a16-8cb5-afdb9c0201ec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#papermill_description=Glob_Log_Files\n",
    "log_files  = pathlib.Path(Logs_input).glob(\"*.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Note: To increase the difficulty of the classification problem, remove occurrences of the phrases \"All assertions passed\", \"Assertion failed\" from buffer\n",
    "#      and write buffer back to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Determine_Pass_or_Fail_Status_For_Each_Deployment_File\n",
    "_FILE_BUF = \"\"\n",
    "pass_deployments = []\n",
    "failed_deployments = []\n",
    "for _file in sorted((log_files)):\n",
    "    with _file.open() as fd:\n",
    "        _FILE_BUF = fd.read()\n",
    "        if \"All assertions passed\" in _FILE_BUF and \"Assertion failed\" not in _FILE_BUF:\n",
    "            pass_deployments.append(_file)\n",
    "        else:\n",
    "            failed_deployments.append(_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Print_Pass_Deployment_Counts\n",
    "len(pass_deployments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Print_Failed_Deployment_Counts\n",
    "len(failed_deployments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Sort_Pass_Deployments_Into_Train_or_Test_Directories\n",
    "import random\n",
    "for _file in pass_deployments:\n",
    "    train_or_test_group = random.choices(population=['train','test'], weights=[80,20]).pop()\n",
    "    _new_file_name = _file.name.replace(\".log\",\".txt\")\n",
    "    _new_file = f\"{sorted_deployment_log_paths[train_or_test_group]['pass']}/{_new_file_name}\"\n",
    "    _file.rename(_new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=Sort_Fail_Deployments_Into_Train_or_Test_Directories\n",
    "import random\n",
    "for _file in failed_deployments:\n",
    "    train_or_test_group = random.choices(population=['train','test'], weights=[80,20]).pop()\n",
    "    _new_file_name = _file.name.replace(\".log\",\".txt\")\n",
    "    _new_file = f\"{sorted_deployment_log_paths[train_or_test_group]['fail']}/{_new_file_name}\"\n",
    "    _file.rename(_new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=train_and_test_files\n",
    "VOCAB_SIZE = 10000\n",
    "TRAIN_FILE_NAMES = []\n",
    "TEST_FILE_NAMES = []\n",
    "TRAIN_FILE_NAMES.extend(pathlib.Path(Logs_input+\"/train/pass\").glob(\"*.txt\"))\n",
    "TRAIN_FILE_NAMES.extend(pathlib.Path(Logs_input+\"/train/fail\").glob(\"*.txt\"))\n",
    "\n",
    "#papermill_description=def_create_model\n",
    "def create_model(vocab_size, num_labels, vectorizer=None):\n",
    "  my_layers =[]\n",
    "  if vectorizer is not None:\n",
    "    my_layers = [vectorizer]\n",
    "\n",
    "  my_layers.extend([\n",
    "      layers.Embedding(vocab_size, 64, mask_zero=True),\n",
    "      layers.Dropout(0.5),\n",
    "      layers.Conv1D(64, 5, padding=\"valid\", activation=\"relu\", strides=2),\n",
    "      layers.GlobalMaxPooling1D(),\n",
    "      layers.Dense(num_labels)\n",
    "  ])\n",
    "\n",
    "  model = tf.keras.Sequential(my_layers)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "\n",
    "Previously, with `tf.keras.utils.text_dataset_from_directory` all contents of a file were treated as a single example. Here, you will use `tf.data.TextLineDataset`, which is designed to create a `tf.data.Dataset` from a text file where each example is a line of text from the original file. `TextLineDataset` is useful for text data that is primarily line-based (for example, poetry or error logs).\n",
    "\n",
    "Iterate through these files, loading each one into its own dataset. Each example needs to be individually labeled, so use `Dataset.map` to apply a labeler function to each one. This will iterate over every example in the dataset, returning (`example, label`) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=define_labeler\n",
    "def labeler(example, index):\n",
    "  return example, tf.cast(index, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=read_text_line_dataset\n",
    "labeled_data_sets = []\n",
    "\n",
    "for i, file_name in enumerate(TRAIN_FILE_NAMES):\n",
    "  lines_dataset = tf.data.TextLineDataset(file_name)\n",
    "  labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))\n",
    "  labeled_data_sets.append(labeled_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you'll combine these labeled datasets into a single dataset using `Dataset.concatenate`, and shuffle it with `Dataset.shuffle`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=set_buffer_size\n",
    "BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "VALIDATION_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=all_labeled_data_concatenate\n",
    "all_labeled_data = labeled_data_sets[0]\n",
    "for labeled_dataset in labeled_data_sets[1:]:\n",
    "  all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\n",
    "\n",
    "all_labeled_data = all_labeled_data.shuffle(\n",
    "    BUFFER_SIZE, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out a few examples as before. The dataset hasn't been batched yet, hence each entry in `all_labeled_data` corresponds to one data point:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, label in all_labeled_data.take(10):\n",
    "  print(\"Log Line: \", text.numpy())\n",
    "  print(\"Label:\", label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset for training\n",
    "\n",
    "Instead of using `tf.keras.layers.TextVectorization` to preprocess the text dataset, you will now use the lower-level TensorFlow Text APIs to standardize and tokenize the data, build a vocabulary and use `tf.lookup.StaticVocabularyTable` to map tokens to integers to feed to the model. (Learn more about [TensorFlow Text](https://www.tensorflow.org/text)).\n",
    "\n",
    "Define a function to convert the text to lower-case and tokenize it:\n",
    "\n",
    "- TensorFlow Text provides various tokenizers. In this example, you will use the `text.UnicodeScriptTokenizer` to tokenize the dataset.\n",
    "- You will use `Dataset.map` to apply the tokenization to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=define_jannahbootlogtokenizer\n",
    "class JannahBootLogTokenizer(tf.keras.layers.Layer):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.tokenizer = tf_text.UnicodeScriptTokenizer()\n",
    "\n",
    "  def call(self, text):\n",
    "    lower_case = tf_text.case_fold_utf8(text)\n",
    "    result = self.tokenizer.tokenize(lower_case)\n",
    "    # If you pass a batch of strings, it will return a RaggedTensor.\n",
    "    if isinstance(result, tf.RaggedTensor):\n",
    "      # Convert to dense 0-padded.\n",
    "      result = result.to_tensor()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=instantiate_jannahbootlogtokenizer\n",
    "tokenizer = JannahBootLogTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=tokenized_ds\n",
    "tokenized_ds = all_labeled_data.map(lambda text, label: (tokenizer(text), label))\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can iterate over the dataset and print out a few tokenized examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=print_tokens\n",
    "for tokens, label in tokenized_ds.take(1):\n",
    "  break\n",
    "\n",
    "print(tokens)\n",
    "print()\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will build a vocabulary by sorting tokens by frequency and keeping the top `VOCAB_SIZE` tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=build_vocabulary \n",
    "import collections\n",
    "tokenized_ds = tokenized_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "vocab_count = collections.Counter()\n",
    "for toks, labels in tokenized_ds.ragged_batch(1000):\n",
    "  toks = tf.reshape(toks, [-1])\n",
    "  for tok in toks.numpy():\n",
    "    vocab_count[tok] += 1\n",
    "\n",
    "vocab = [tok for tok, count in vocab_count.most_common(VOCAB_SIZE)]\n",
    "\n",
    "print(\"First five vocab entries:\", vocab[:5])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the tokens into integers, use the `vocab` set to create a `tf.lookup.StaticVocabularyTable`. You will map tokens to integers with `0` reserved for padding, and `n+1` reserved to denote an out-of-vocabulary (OOV) token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=define_vocabulary_table\n",
    "class JannahBootLogsVocabTable(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab):\n",
    "    super().__init__()\n",
    "    self.keys = [''] + vocab\n",
    "    self.values = range(len(self.keys))\n",
    "\n",
    "    self.init = tf.lookup.KeyValueTensorInitializer(\n",
    "        self.keys, self.values, key_dtype=tf.string, value_dtype=tf.int64)\n",
    "\n",
    "    num_oov_buckets = 1\n",
    "\n",
    "    self.table = tf.lookup.StaticVocabularyTable(self.init, num_oov_buckets)\n",
    "\n",
    "  def call(self, x):\n",
    "    result = self.table.lookup(x)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it on a dummy vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=instantiate_vocabulary_table_with_dummy_data\n",
    "vocab_table = JannahBootLogsVocabTable(['a','b','c'])\n",
    "vocab_table(tf.constant([''] + list('abcdefghi')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create one for the real vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=instantiate_vocabulary_table_with_log_data\n",
    "vocab_table = JannahBootLogsVocabTable(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, define a layer to standardize, tokenize and vectorize the dataset using the tokenizer and lookup table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=define_layer_to_standardize_tokenize_and_vectorize_the_dataset\n",
    "preprocess_text = tf.keras.Sequential([\n",
    "    tokenizer,\n",
    "    vocab_table\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try this on a single example to print the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=define_layer_to_standardize_tokenize_and_vectorize_the_dataset\n",
    "example_text, example_label = next(iter(all_labeled_data))\n",
    "print(\"Log Line: \", example_text.numpy())\n",
    "vectorized_text = preprocess_text(example_text)\n",
    "print(\"Vectorized log line: \", vectorized_text.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a dataset pipeline that will process the text on the fly using `Dataset.map`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=print_all_encoded_data\n",
    "all_encoded_data = all_labeled_data.map(lambda text, labels:(preprocess_text(text), labels))\n",
    "\n",
    "for ids, label in all_encoded_data.take(1):\n",
    "  break\n",
    "\n",
    "print(\"Ids: \", ids.numpy())\n",
    "print(\"Label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Keras `TextVectorization` layer also batches and pads the vectorized data. Padding is required because the examples inside of a batch need to be the same size and shape, but the examples in these datasets are not all the same size—each line of text has a different number of words.\n",
    "\n",
    "`tf.data.Dataset` supports splitting and padded-batching datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=split_the_dataset_into_training_and_test_sets\n",
    "train_data = all_encoded_data.skip(VALIDATION_SIZE).shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
    "validation_data = all_encoded_data.take(VALIDATION_SIZE).padded_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `validation_data` and `train_data` are not collections of (`example, label`) pairs, but collections of batches. Each batch is a pair of (*many examples*, *many labels*) represented as arrays.\n",
    "\n",
    "To illustrate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=print_text_batch_label_batch\n",
    "sample_text, sample_labels = next(iter(validation_data))\n",
    "print(\"Text batch shape: \", sample_text.shape)\n",
    "print(\"Label batch shape: \", sample_labels.shape)\n",
    "print(\"First text example: \", sample_text[0])\n",
    "print(\"First label example: \", sample_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the datasets for better performance as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#papermill_description=autotune_train_data_validation_data\n",
    "train_data = train_data.prefetch(tf.data.AUTOTUNE)\n",
    "validation_data = validation_data.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "You can train a model on this dataset as before:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this text vectorization adds `0` for padding and `n+1` for out-of-vocabulary (OOV) tokens, the vocabulary size has increased by two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=create_and_compile_model\n",
    "model = create_model(vocab_size=VOCAB_SIZE+2, num_labels=300)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=plot_model\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=load_previously_saved_model_if_any\n",
    "import os\n",
    "if os.path.isdir(\"export.tf\"): \n",
    "    print(\"read previously saved model\")\n",
    "    loaded = tf.saved_model.load('export.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=fit_model\n",
    "history = model.fit(train_data, validation_data=validation_data, epochs=_MODEL_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=evaluate_model\n",
    "metrics = model.evaluate(validation_data, return_dict=True)\n",
    "\n",
    "print(\"Loss: \", metrics['loss'])\n",
    "print(\"Accuracy: {:2.2%}\".format(metrics['accuracy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ANSIBLE_VERBOSE_LEVEL=$_ANSIBLE_VERBOSE_LEVEL make the model capable of taking raw strings as input, pack both the text processor and the model into a `keras.Sequential`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=sequential_model\n",
    "export_model = tf.keras.Sequential([\n",
    "    preprocess_text,\n",
    "    model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=sequential_model_compile\n",
    "export_model.compile(\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model you can run directly on batches of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=create_a_test_dataset_of_raw_strings.\n",
    "test_ds = all_labeled_data.take(VALIDATION_SIZE).batch(BATCH_SIZE)\n",
    "test_ds = test_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=sequential_model\n",
    "loss, accuracy = export_model.evaluate(test_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: {:2.2%}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `saved_model.save` to export it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=sequential_model_save\n",
    "tf.saved_model.save(export_model, 'export.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=sequential_model_export\n",
    "loaded = tf.saved_model.load('export.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=sequential_model_export\n",
    "export_model(tf.constant(['TASK [Kubeflow install (standalone deployment) docker-desktop cluster - wait for crd/applications.app.k8s.io] ***'])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss and accuracy for the model on encoded validation set and the exported model on the raw validation set are the same, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papermill_description=run_inference_on_new_data\n",
    "inputs = [\n",
    "    \"TASK [Git checkout https://github.com/jannahio/frontend.git] *******************\",  # Label: 1\n",
    "    \"TASK [Create images/ubuntu directory] ******************************************\",  # Label: 2\n",
    "    \"TASK [Import cleanup prepared build image tasks - streamos middleware and frontend] ***\",  # Label: 0\n",
    "]\n",
    "\n",
    "predicted_scores = export_model.predict(inputs)\n",
    "predicted_labels = tf.math.argmax(predicted_scores, axis=1)\n",
    "\n",
    "for input, label in zip(inputs, predicted_labels):\n",
    "  print(\"Log Line: \", input)\n",
    "  print(\"Predicted label: \", label.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
